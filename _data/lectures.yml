- date: Thu 08/31/23
  lecturer:
    - Arman
  title:
    - Course introduction
    - Logistics
    - Transformers - high level overview
  slides: https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EasWFaRb3O9FlIjBXGg2jpYBlXuQ6pfCnr9kVorcUrP4kw?e=aJnoui
  readings:
    - Attention is all you need (2017) <a href="https://arxiv.org/abs/1706.03762" target="_blank">[link]</a>
  logistics:

- date: Tues 09/05/23
  lecturer:
    - Arman
  title:
    - Optimization, backpropagation, and training
  slides: https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EY-tqM8VeRxGpNbdiyWnb0oBGcMZhix4In9a29nPdZpU4Q?e=1lDpDQ
  # slides2:
  logistics:
  readings:
    - Deep Feedforward Networks. Ian Goodfellow, Yoshua Bengio, & Aaron Courville (2016). Deep Learning, Chapter 6.5. <a href="https://www.deeplearningbook.org/contents/mlp.html" target="_blank">[link]</a>
    - An overview of gradient descent optimization algorithms <a href="https://arxiv.org/pdf/1609.04747.pdf" target="_blank">[link]</a>
    - A Gentle Introduction to Torch Autograd <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">[link]</a>
    - Autograd Mechanics <a href="https://pytorch.org/docs/stable/notes/autograd.html">[link]</a>

- date: Thu 09/07/23
  lecturer:
    - Arman
  title:
    - Word embeddings
    - Tokenization
  readings:
    - Distributed Representations of Words and Phrases and their Compositionality (2013) <a href="https://arxiv.org/pdf/1310.4546.pdf" target="_blank">[link]</a>
    - GloVe- Global Vectors for Word Representation (2014) <a href="https://nlp.stanford.edu/pubs/glove.pdf" target="_blank">[link]</a>
    - BPE - Neural Machine Translation of Rare Words with Subword Units (2016) <a href="https://arxiv.org/pdf/1508.07909.pdf" target="_blank">[link]</a>
    - SentencePiece- A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (2018) <a href="https://arxiv.org/pdf/1808.06226.pdf" target="_blank">[link]</a>
  slides: https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EeUZYaR-GFRHldOt3Ss9hv8BsxW6SVuZC5zs-oG79eq1cQ?e=d6JVXL
  logistics:

- date: Tue 09/12/23
  lecturer:
    - Arman
  title:
    - Transformers
    - Implementation details
  readings:
    - Attention is all you need (2017) <a href="https://arxiv.org/abs/1706.03762" target="_blank">[link]</a>
    - The Annotated Transformer <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank">[link]</a>
  slides: https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/Ec_DoGtFQNhBl61i8JEbEK4BVIY3tmfiAfXyVJtj1IVePA?e=JesZaahttps://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/Ec_DoGtFQNhBl61i8JEbEK4BVIY3tmfiAfXyVJtj1IVePA?e=8N74Ox
  logistics:

- date: Thu 09/14/23
  lecturer:
    - Arman
  title:
    - Positional Information
      - Absolute
      - Relative
      - ROPE
      - ALiBi
    - Multi-Query Attention
    - Grouped Multi-Query Attention
    - Inference
    - KV caching
    - Encoder/decoder-only vs encoder-decoder
  notes:
  readings:
    - Train Short, Test Long- Attention with Linear Biases Enables Input Length Extrapolation <a href="https://arxiv.org/abs/2108.12409" target="_blank">[link]</a>
    - RoFormer- Enhanced Transformer with Rotary Position Embedding <a href="https://arxiv.org/pdf/2104.09864.pdf" target="_blank">[link]</a>
    - Fast Transformer Decoding- One Write-Head is All You Need <a href="https://arxiv.org/pdf/1911.02150.pdf">[link]</a>
  slides: https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EQZlgr48VPRPo6fG7QUtYjABmxvnsCVxp-tO-RFmdhnQlQ?e=fkZVHV
  logistics: Tips on choosing a project <a href="https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/Ea6n3iOmH01AltSbPq7TbmcBNYwAn66Ww6k5VQWYwxyqVg?e=sqfkct" target="_blank">[slides]</a> <br/><br/> HW1 out <a href="/homework">[link]</a>

- date: Th 09/19/23
  lecturer:
    - Arman
  title:
    - Transfer Learning (continued)
    - Transformer efficiency
  readings:
    - ELMo, Deep Contextualized Representations <a href="https://arxiv.org/pdf/1802.05365.pdf">[link]</a>
    - ULMFit, Universal Language Model Fine-tuning for Text Classification <a href="https://arxiv.org/pdf/1801.06146.pdf">[link]</a>
    - BERT, Pre-training of Deep Bidirectional Transformers for Language Understanding <a href="https://arxiv.org/pdf/1810.04805.pdf">[link]</a>
    - UL2- Unifying Language Learning Paradigms (2022) <a href="https://arxiv.org/pdf/2205.05131.pdf">[link]</a>
    - What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? (2022) <a href="https://arxiv.org/pdf/2204.05832.pdf">[link]</a>
    - BART, Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension <a href="https://arxiv.org/pdf/1910.13461.pdf">[link]</a>
    - ELECTRA, Pre-training Text Encoders as Discriminators Rather Than Generators <a href="https://arxiv.org/abs/2003.10555">[link]</a>
  logistics:
    - Project teams due. <a href="https://forms.gle/Ce37NzChwvbX2N5Y6">[Team submission form]</a>

- date: Th 09/21/23
  lecturer:
    - Arman
  title:
    - Transformer efficiency
    - Hardware aware implementation
  readings:
    - Flash attention - Fast and memory-efficient exact attention with IO-awareness (2022) <a href="https://arxiv.org/pdf/2205.14135.pdf">[link]</a>
    - Flash attention v2 - Faster Attention with Better Parallelism and Work Partitioning

- date: Tue 09/26/23
  lecturer:
    - Arman
  title:
    - Scale
    - GPT3
    - Emergeing properties
  readings:
    - Scaling Laws for Neural Language Models <a href="https://arxiv.org/pdf/2001.08361.pdf" target="_blank">[link]</a>
    - Training Compute-Optimal Large Language Models <a href="https://arxiv.org/pdf/2203.15556.pdf" target="_blank">[link]</a>
    - Chain of Thought Prompting Elicits Reasoning in Large Language Models <a href="https://arxiv.org/pdf/2201.11903.pdf" target="_blank">[link]</a>

- date: Thu 09/28/23
  lecturer:
    - TBA
  title:
    - Parameter efficient fine tuning
  readings:
    - Parameter-Efficient Transfer Learning for NLP (2019) <a href="https://arxiv.org/pdf/1902.00751.pdf" target="_blank">[link]</a>
    - Prefix-Tuning- Optimizing Continuous Prompts for Generation (2021) <a href="https://arxiv.org/pdf/2101.00190.pdf" target="_blank">[link]</a>
    - Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning (2022) <a href="https://arxiv.org/pdf/2205.05638.pdf" target="_blank">[link]</a>
    - LoRA- Low-Rank Adaptation of Large Language Models <a href="https://arxiv.org/pdf/2106.09685.pdf" target="_blank">[link]</a>

- date: Tue 10/03/23
  lecturer:
    - Arman
  title:
    - Instruction tuning
  readings:
    - Training language models to follow instructions with human feedback (2022) <a href="https://arxiv.org/pdf/2203.02155.pdf" traget="_blank">[link]</a>
    - Scaling Instruction-Finetuned Language Models (2022) <a href="https://arxiv.org/pdf/2210.11416.pdf" traget="_blank">[link]</a>

- date: Thu 10/05/23
  lecturer:
    - Arman
  title:
    - Adaptation
    - Reinforcement Learning for language models fine-tuning
  readings:
    - Fine-Tuning Language Models from Human Preferences (2019) <a href="https://arxiv.org/abs/1909.08593" traget="_blank">[link]</a>
    - Learning to Summarize with Human Feedback (2020) <a href="https://arxiv.org/pdf/2009.01325.pdf" traget="_blank">[link]</a>
    - InstructGPT - Fine-Tuning Language Models from Human Instructions (2021) <a href="https://arxiv.org/pdf/2109.01196.pdf" traget="_blank">[link]</a>
    - Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback (2022) <a href="https://arxiv.org/pdf/2204.05862.pdf" traget="_blank">[link]</a>
    - Direct Preference Optimization- Your Language Model is Secretly a Reward Model (2023) <a href="https://arxiv.org/pdf/2305.18290.pdf" traget="_blank">[link]</a>
  optional:
    - Parameter-Efficient Transfer Learning for NLP (2019) <a href="https://arxiv.org/pdf/1902.00751.pdf" traget="_blank">[link]</a>
    - Prefix-Tuning- Optimizing Continuous Prompts for Generation (2021) <a href="https://arxiv.org/pdf/2101.00190.pdf" traget="_blank">[link]</a>
  logistics:
    - HW 1 due

- date: Tues 10/10/23
  lecturer:
    - Arman
  title:
    - Guest lecture by <a href="https://beltagy.net/" target="_blank">Iz Beltagy</a> <br/>Building an Open Source LLM

- date: Thu 10/12/23
  lecturer:
    - Arman
  title:
    - Mixture of experts
  readings:
    - Outrageously Large Neural Networks- The Sparsely-Gated Mixture-of-Experts Layer (2017) <a href="https://arxiv.org/pdf/1701.06538.pdf" traget="_blank">[link]</a>
    - Switch Transformers- Scaling to Trillion Parameter Models (2021) <a href="https://arxiv.org/pdf/2101.03961.pdf" traget="_blank">[link]</a>
    - Sparse Upcycling- Training Mixture-of-Experts from Dense Checkpoints (2022) <a href="https://arxiv.org/pdf/2212.05055.pdf" traget="_blank">[link]</a>

  optional:
    - Efficient Transformers- A Survey <a href="https://arxiv.org/pdf/2009.06732.pdf" target="_blank">[link] </a>

- date: Tue 10/17/23
  title:
    - Guest lecture by <a href="https://scholar.google.com/citations?hl=en&user=_8mkIjgAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Tushar Khot</a> <br/>Tool augmented language models, Modular networks
  readings:
    - Toolformer - A Tool-Augmented Language Model (2022) <a href="https://arxiv.org/pdf/2209.01696.pdf" traget="_blank">[link]</a>
  logistics:
    - 10/19 Project proposal due

- date: 10/17/23 - 10/23/23
  lecturer:
  title: >
    <strong> October recess - No classes </strong>
  recitation:

- date: Tue 10/24/23
  lecturer:
    - Arman
  title:
    - Evaluation
  readings:
    - Holistic Evaluation of Large Language Models for Code Generation (2022) <a href="https://arxiv.org/pdf/2211.09110.pdf" traget="_blank">[link]</a>

- date: Thu 10/26/23
  lecturer:
  title: >
    <strong> Midterm </strong>
  recitation:

- date: Tue 10/31/23
  lecturer: Arman
  title: Guest lecture by <a href="https://shmsw25.github.io" target="_blank">Sewon Min</a> <br/>Retrieval augmented language models
  readings:
    - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (2020) <a href="https://arxiv.org/pdf/2005.11401.pdf" traget="_blank">[link]</a>
    - Improving language models by retrieving from trillions of tokens (2021) <a href="https://arxiv.org/pdf/2112.04426.pdf" traget="_blank">[link]</a>
    - REPLUG, Retrieval-Augmented Black-Box Language Models <a href="https://arxiv.org/pdf/2301.12652.pdf" target="_blank">[link]</a>

- date: Tue 11/02/23
  lecturer: Arman
  title:
    - Guest lecture by <a href="https://colinraffel.com/" target="_blank">Colin Raffel</a><br/> Build an Ecosystem, Not a Monolith<br/>
  readings:
    - Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning (2022) <a href="https://arxiv.org/pdf/2205.05638.pdf" traget="_blank">[link]</a>
    - Exploring and Predicting Transferability across NLP Tasks (2020) <a href="https://aclanthology.org/2020.emnlp-main.635.pdf" traget="_blank">[link]</a>
    - Editing Models with Task Arithmetic (2022) <a href="https://arxiv.org/pdf/2212.04089.pdf" traget="_blank">[link]</a>

- date: Tue 11/07/23
  lecturer: Arman
  title:
    - Quantization
    - Distillation
  readings:
    - LLM.int8()- 8-bit Matrix Multiplication for Transformers at Scale (2022) <a href="https://arxiv.org/abs/2208.07339" traget="_blank">[link]</a>
  logistics:
    - HW 2 due

- date: Thu 11/09/23
  title:
    - Safety, security, and societal considerations
  readings:
    - On the dangers of stochastic parrots- Can language models be too big? (2021) <a href="https://arxiv.org/pdf/2005.14050.pdf" traget="_blank">[link]</a>

- date: Tue 11/14/23
  lecturer: Arman
  title:
    - Contrastive learning
    - Pretrained vision models
  readings:
    - A Simple Framework for Contrastive Learning of Visual Representations (2020) <a href="https://arxiv.org/pdf/2002.05709.pdf" traget="_blank">[link]</a>
    - SwAV- Unsupervised Learning of Visual Features by Contrasting Cluster Assignments (2021) <a href="https://arxiv.org/pdf/2006.09882.pdf" traget="_blank">[link]</a>
    - MoCo - Momentum Contrast for Unsupervised Visual Representation Learning (2020) <a href="https://arxiv.org/pdf/1911.05722.pdf" traget="_blank">[link]</a>

- date: Tue 11/16/23
  lecturer: Arman
  title:
    - Vision transformers
  readings:
    - An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale (2020) <a href="https://arxiv.org/pdf/2010.11929.pdf" traget="_blank">[link]</a>
    - Training data-efficient image transformers & distillation through attention (2021) <a href="https://arxiv.org/pdf/2012.12877.pdf" traget="_blank">[link]</a>
  logistics:
    - 11/17 Project progress report due

- date: 11/17/23 - 11/26/23
  lecturer:
  title: >
    <strong> Thanksgiving recess - No classes </strong>
  recitation:

- date: Tue 11/28/23
  lecturer: Arman
  title:
    - Scaling vision transformer models
  readings:
    - Scaling Vision Transformers to 22 Billion Parameters <a href="https://arxiv.org/pdf/2302.05442.pdf" traget="_blank">[link]</a>

- date: Thu 11/30/23
  lecturer: Arman
  title:
    - Multi-modal models
    - Speech and audio
  readings:
    - CM3 - A Causal Masked Multimodal Model of the Internet (2022) <a href="https://arxiv.org/pdf/2201.07520.pdf" traget="_blank">[link]</a>
    - Flamingo- a Visual Language Model for Few-Shot Learning (2022) <a href="https://arxiv.org/pdf/2204.14198.pdf" traget="_blank">[link]</a>
    - Prompting Large Language Models with Speech Recognition Abilities (2023) <a href="https://arxiv.org/pdf/2307.11795.pdf" traget="_blank">[link]</a>
  logistics:
    - 12/1 HW 3 due

- date: 12/05/23
  lecturer: Arman
  title:
    - Foundation models for Code generation
  readings:
    - StarCoder- May the source be with you! (2023) <a href="https://arxiv.org/pdf/2305.06161.pdf" traget="_blank">[link]</a>

- date: 12/07/23
  lecturer: Arman
  title:
    - TBA
  logistics:
    - 12/18 Final project report due
