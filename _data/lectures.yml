- date: Thu 08/31/23
  lecturer:
    - Arman
  title:
    - Course introduction
    - Logistics
    - Transformers
  slides:
  readings:
    - Attention is all you need (2017) <a href="https://arxiv.org/abs/1706.03762" target="_blank">[link]</a>
  logistics:

- date: Tues 09/05/23
  lecturer:
    - Arman
  title:
    - Transfer learning
    - Pre-training
    - Pre-trained transformers
  slides:
  # slides2:
  logistics:
  readings:
    - ELMo, Deep Contextualized Representations <a href="https://arxiv.org/pdf/1802.05365.pdf">[link]</a>
    - ULMFit, Universal Language Model Fine-tuning for Text Classification <a href="https://arxiv.org/pdf/1801.06146.pdf">[link]</a>
    - BERT, Pre-training of Deep Bidirectional Transformers for Language Understanding <a href="https://arxiv.org/pdf/1810.04805.pdf">[link]</a>
  optional:
    - <p> RoBERTa, A Robustly Optimized BERT Pretraining Approach <a href="https://arxiv.org/pdf/1907.11692.pdf">[link]</a> </p>

- date: Thu 09/07/23
  lecturer:
    - Arman
  # question-form: https://forms.gle/EBNHeejzviX5pKvz5
  title:
    - Transfer learning
    - Pre-training
  readings:
    - Language Models are Unsupervised Multitask Learners (GPT-2) <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf " target="_blank">[link]</a>
    - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5) <a href="https://arxiv.org/pdf/1910.10683.pdf">[link]</a>
  optional:
    - BART, Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension <a href="https://arxiv.org/pdf/1910.13461.pdf">[link]</a>
    - ELECTRA, Pre-training Text Encoders as Discriminators Rather Than Generators <a href="https://arxiv.org/abs/2003.10555">[link]</a>
  logistics:

- date: Tue 09/12/23
  lecturer:
    - Arman
  title: Model Architecture and Training Objectives
  # slides: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_4_ul2.pdf
  # question-form: https://forms.gle/wqjDZ8cj3Buuu6yt8
  readings:
    - UL2- Unifying Language Learning Paradigms (2022) <a href="https://arxiv.org/pdf/2205.05131.pdf">[link]</a>
    - What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? (2022) <a href="https://arxiv.org/pdf/2204.05832.pdf">[link]</a>
  logistics:

- date: Thu 09/14/23
  lecturer:
    - Arman
  title:
    - Scaling Laws for Langauge Models
  notes:
  readings:
    - Scaling Laws for Neural Language Models (2020) <a href="https://arxiv.org/pdf/2001.08361.pdf " target="_blank">[link]</a>
    - Training Compute-Optimal Large Language Models (2022) <a href="https://arxiv.org/pdf/2203.15556.pdf" target="blank">[link]</a>
  logistics: <a href="https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_5_projects.pdf"> [Tips on choosing projects]</a> <br/><br/> <a href="https://docs.google.com/document/d/1JBCkDdlywQcQtxz6fHg1p2ev0GgLbmHbcAg4NRIu-Mc/edit">[Projects Doc]</a>
  optional:
    - Scale Efficiently - Insights from Pre-training and Fine-tuning Transformers (2021) <a href="https://arxiv.org/pdf/2109.10686.pdf">[link]</a>

- date: Th 09/19/23
  lecturer:
    - Arman
  title:
    - LLMs
    - Power of scale
  readings:
    - Language Models are Few-Shot Learners (GPT3; 2020) <a href="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" target="_blank">[link]</a>
    - PaLM - Scaling Language Modeling with Pathways (2022) <a href="https://arxiv.org/pdf/2204.02311.pdf">[link]</a>
  # slides: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_6_gpt3.pdf
  # slides2: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_6_palm.pdf

- date: Tue 09/26/23
  lecturer:
    - TBA
  title:
    - Guest lecture 1
  readings: TBA
  # question-form: https://forms.gle/tPn1iGLN6Cp88TXo7
  # slides: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_7_pet.pdf
  # slides2: https://docs.google.com/presentation/d/1_VGuTUS-JvmOTRwoXNPLUA3ykkBfJWU12UG27zUMGls/edit

- date: Thu 09/28/23
  lecturer:
    - TBA
  title:
    - Guest lecture 2
  readings:
    - Rethinking the Role of Demonstrations- What Makes In-Context Learning Work? (2022) <a href="https://arxiv.org/pdf/2202.12837.pdf" traget="_blank">[link]</a>
    - Data Distributional Properties Drive Emergent In-Context Learning in Transformers (2022) <a href="https://arxiv.org/pdf/2205.05055.pdf" traget="_blank">[link]</a>
  optional:
    - What learning algorithm is in-context learning? Investigations with linear models (2022) <a href="https://arxiv.org/pdf/2211.15661.pdf" target="_blank">[link]</a>
    - Transformers as Algorithms- Generalization and Stability in In-context Learning (2023) <a href="https://arxiv.org/pdf/2301.07067.pdf" target="_blank">[link]</a>

- date: Tue 10/03/23
  lecturer:
    - Arman
  title:
    - Instruction tuning
  readings:
    - Training language models to follow instructions with human feedback (2022) <a href="https://arxiv.org/pdf/2203.02155.pdf" traget="_blank">[link]</a>
    - Scaling Instruction-Finetuned Language Models (2022) <a href="https://arxiv.org/pdf/2210.11416.pdf" traget="_blank">[link]</a>
  # question-form: https://forms.gle/pSixeeAx6GKCsCGBA

- date: Thu 10/05/23
  lecturer:
    - Arman
  title:
    - Parameter efficient fine-tuning
  readings:
    - Towards a Unified View of Parameter-Efficient Transfer Learning (2021) <a href="https://arxiv.org/pdf/2110.04366.pdf" traget="_blank">[link]</a>
    - Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning (2022) <a href="https://arxiv.org/pdf/2205.05638.pdf" traget="_blank">[link]</a>
  optional:
    - Parameter-Efficient Transfer Learning for NLP (2019) <a href="https://arxiv.org/pdf/1902.00751.pdf" traget="_blank">[link]</a>
    - Prefix-Tuning- Optimizing Continuous Prompts for Generation (2021) <a href="https://arxiv.org/pdf/2101.00190.pdf" traget="_blank">[link]</a>

- date: Tues 10/10/23
  lecturer:
    - Arman
  title:
    - Chain of thought reasoning
    - Self-consistency
    - Emergence
  readings:
    - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (2022) <a href="https://arxiv.org/pdf/2201.11903.pdf" traget="_blank">[link]</a>
    - Emergent Abilities of Large Language Models (2022) <a href="https://openreview.net/pdf?id=yzkSU5zdwD" traget="_blank">[link]</a>

- date: Thu 10/12/23
  lecturer:
    - Arman
  title:
    - Efficient transformers
  readings:
    - Longformer- The Long-Document Transformer (2020) <a href="https://arxiv.org/pdf/2004.05150.pdf" traget="_blank">[link]</a>
    - Big Bird- Transformers for Longer Sequences (2020) <a href="https://arxiv.org/pdf/2007.14062.pdf" traget="_blank">[link]</a>
  optional:
    - Efficient Transformers- A Survey <a href="https://arxiv.org/pdf/2009.06732.pdf" target="_blank">[link] </a>

- date: Tue 10/17/23
  lecturer:
    - Arman
  title:
    - Efficient transformers
  readings:
    - Efficiently Modeling Long Sequences with Structured State Spaces (2021) <a href="https://arxiv.org/pdf/2111.00396.pdf" traget="_blank">[link]</a>
    - Simplifying S4 <a href="https://hazyresearch.stanford.edu/blog/2022-06-11-simplifying-s4" target="_blank">[link]</a>
  optional:
    - On the Parameterization and Initialization of Diagonal State Space Models (2022) <a href="https://arxiv.org/pdf/2206.11893.pdf" traget="_blank">[link]</a>
    - Hungry Hungry Hippos- Towards Language Modeling with State Space Models (2022) <a href="https://arxiv.org/pdf/2212.14052.pdf" traget="_blank">[link]</a>
    - FlashAttention- Fast and Memory-Efficient Exact Attention with IO-Awareness (2022) <a href="https://arxiv.org/pdf/2205.14135.pdf" traget="_blank">[link]</a>

- date: 10/17/23 - 10/23/23
  lecturer:
  title: >
    <strong> October recess - No classes </strong>
  recitation:

- date: Tue 10/24/23
  lecturer:
    - Arman
  title:
    - Memory
  readings:
    - Memorizing Transformers (2022) <a href="https://arxiv.org/pdf/2203.08913.pdf" traget="_blank">[link]</a>
  optional:
    - Training Language Models with Memory Augmentation (2022) <a href="https://arxiv.org/pdf/2205.12674.pdf" traget="_blank">[link]</a>
  logistics:
    - <span class="deadline">Project proposals due on 3/4</span>
  question-form: https://forms.gle/6GUGN4ABLqAMQoWh9

- date: Thu 10/26/23
  title: >
    <strong> Midterm </strong>
  recitation:

- date: Tue 10/31/23
  lecturer: Arman
  title: Retrieval Augmented Language Models
  readings:
    - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (2020) <a href="https://arxiv.org/pdf/2005.11401.pdf" traget="_blank">[link]</a>
    - Improving language models by retrieving from trillions of tokens (2021) <a href="https://arxiv.org/pdf/2112.04426.pdf" traget="_blank">[link]</a>
    - REPLUG, Retrieval-Augmented Black-Box Language Models <a href="https://arxiv.org/pdf/2301.12652.pdf" target="_blank">[link]</a>
    - Retrieval-Augmented Multimodal Language Modeling <a href="https://arxiv.org/pdf/2211.12561.pdf" target="_blank">[link]</a>

- date: Thu 11/02/23
  lecturer: Arman
  title:
    - Tool augmented language models
  readings:
    - PEER- A Collaborative Language Model (2022) <a href="https://arxiv.org/pdf/2208.11663.pdf" traget="_blank">[link]</a>
  optional:
    - Generating Sequences by Learning to Self-Correct (2022) <a href="https://arxiv.org/pdf/2211.00053.pdf" traget="_blank">[link]</a>

- date: Tue 11/07/23
  lecturer: Arman
  title:
    - Mixture of experts and sparse models
  readings:
    - Switch Transformers- Scaling to Trillion Parameter Models (2021) <a href="https://arxiv.org/pdf/2101.03961.pdf" traget="_blank">[link]</a>
  optional:
    - Sparse Upcycling- Training Mixture-of-Experts from Dense Checkpoints (2022) <a href="https://arxiv.org/pdf/2212.05055.pdf" traget="_blank">[link]</a>
    # - Hyperdecoders- Instance-specific decoders for multi-task NLP (2022) <a href="https://arxiv.org/pdf/2203.08304.pdf" traget="_blank">[link]</a>

- date: Tue 11/07/23
  lecturer: Arman
  title:
    - Training data
  readings:
    - Competency Problems- On Finding and Removing Artifacts in Language Data (2021) <a href="https://arxiv.org/pdf/2104.08646.pdf" traget="_blank">[link]</a>
  optional:
    - Understanding Dataset Difficulty with V-Usable Information (2021) <a href="https://arxiv.org/pdf/2110.08420.pdf" traget="_blank">[link]</a>

- date: Thu 11/09/23
  lecturer: Arman
  title:
    - Mixture of experts and sparse models
  readings:
    - Switch Transformers- Scaling to Trillion Parameter Models (2021) <a href="https://arxiv.org/pdf/2101.03961.pdf" traget="_blank">[link]</a>
  optional:
    - Sparse Upcycling- Training Mixture-of-Experts from Dense Checkpoints (2022) <a href="https://arxiv.org/pdf/2212.05055.pdf" traget="_blank">[link]</a>
    # - Hyperdecoders- Instance-specific decoders for multi-task NLP (2022) <a href="https://arxiv.org/pdf/2203.08304.pdf" traget="_blank">[link]</a>
  question-form: https://forms.gle/V37Z4vjM6mA6ad397

- date: 11/14/23
  lecturer: Arman
  title:
    - Vision transformers 1

- date: 11/16/23
  lecturer: Arman
  title:
    - Vision transformers 2 and scaling up vision models

- date: 11/17/23 - 11/26/23
  lecturer:
  title: >
    <strong> Thanksgiving recess - No classes </strong>
  recitation:

- date: 11/28/23
  lecturer: Arman
  title:
    - Multi-modal models
  readings:
    # - CM3- A Causal Masked Multimodal Model of the Internet (2022) <a href="https://arxiv.org/pdf/2201.07520.pdf" traget="_blank">[link]</a>
    - Flamingo- a Visual Language Model for Few-Shot Learning (2022) <a href="https://arxiv.org/pdf/2204.14198.pdf" traget="_blank">[link]</a>
  optional:
    - CM3- A Causal Masked Multimodal Model of the Internet (2022) <a href="https://arxiv.org/pdf/2201.07520.pdf" traget="_blank">[link]</a>

- date: 11/30/23
  lecturer: Arman
  title:
    - Speech and audio models

- date: 12/05/23
  lecturer:
  title:
    - Guest lecture

- date: 12/07/23
  lecturer:
  title:
    - Guest lecture

- date: TBD
  lecturer: All students
  title:
    - Final presentations for projects
  # readings:
  #   - LLM.int8()- 8-bit Matrix Multiplication for Transformers at Scale <a href="https://arxiv.org/pdf/2208.07339.pdf" traget="_blank">[link]</a>
  #   - Efficiently Scaling Transformer Inference <a href="https://arxiv.org/pdf/2211.05102.pdf" traget="_blank">[link]</a>
  logistics:
    - <span class="deadline">4/27 Presentations;</span>
# - date:
#   lecturer: Arman
#   title:
#     - Training LLMs with human feedback
#     - AI Alignment
#   readings:
#     # - Learning to Summarize with Human Feedback (2020) <a href="https://arxiv.org/pdf/2009.01325.pdf">[link]</a>
#     - A General Language Assistant as a Laboratory for Alignment (2021) <a href="https://arxiv.org/pdf/2112.00861.pdf" traget="_blank">[link]</a>
#   optional:
#     - Learning to Summarize with Human Feedback (2020) <a href="https://arxiv.org/pdf/2009.01325.pdf">[link]</a>
#   question-form: https://forms.gle/5C3n8ozPZKrsUKRw5

# - date:
#   lecturer: Ziqing
#   title:
#     - AI Alignment
#   readings:
#     # - Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback (2022) <a href="https://arxiv.org/pdf/2204.05862.pdf" traget="_blank">[link]</a>
#     - Fine-tuning language models to find agreement among humans with diverse preferences (2022) <a href="https://arxiv.org/pdf/2211.15006.pdf" traget="_blank">[link]</a>
#   optional:
#     - Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback (2022) <a href="https://arxiv.org/pdf/2204.05862.pdf" traget="_blank">[link]</a>
#   question-form: https://forms.gle/hriZijXM6meicBZ48

# - date:
#   lecturer: Huangrui
#   title:
#     - LLMs for Code
#   readings:
#     - CodeGen- An Open Large Language Model for Code with Multi-Turn Program Synthesis (2022) <a href="https://arxiv.org/pdf/2203.13474.pdf" traget="_blank">[link]</a>
#   optional:
#     - InCoder- A Generative Model for Code Infilling and Synthesis (2022) <a href="https://arxiv.org/pdf/2204.05999.pdf" traget="_blank">[link]</a>
#   question-form: https://forms.gle/U59pWv8J8J5LvTz28

# - date:
#   lecturer:
#   title:
#     - <b> Guest Lecture</b>
#     - <a href="https://timdettmers.com/about/">Tim Dettmers</a> - Making LLMs more efficient
#   readings:
#     - LLM.int8()- 8-bit Matrix Multiplication for Transformers at Scale <a href="https://arxiv.org/pdf/2208.07339.pdf" traget="_blank">[link]</a>
#     - Efficiently Scaling Transformer Inference <a href="https://arxiv.org/pdf/2211.05102.pdf" traget="_blank">[link]</a>

# - date:
#   lecturer: All students
#   title:
#     - Final presentations for projects
#   # readings:
#   #   - LLM.int8()- 8-bit Matrix Multiplication for Transformers at Scale <a href="https://arxiv.org/pdf/2208.07339.pdf" traget="_blank">[link]</a>
#   #   - Efficiently Scaling Transformer Inference <a href="https://arxiv.org/pdf/2211.05102.pdf" traget="_blank">[link]</a>
#   logistics:
#     - <span class="deadline">4/27 Presentations; 5/10 Final project report</span>
